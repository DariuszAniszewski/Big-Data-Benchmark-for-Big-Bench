{"name":"Big-bench","tagline":"BigBench -- Big Data Analytics Hadoop Benchmark.","body":"Welcome to BigBench -- BigBench is a proposed Industry standard Big Data Batch analytics benchmark. This implementation of BigBench is powered by Hadoop Mapreduce, HIVE, Mahout for data processing and HDFS for data storage. \r\n\r\nThis wiki will be primary source of BigBench documentation for users to download and run the benchmark on their Hadoop clusters. \r\n\r\nThe Benchmark kit offered here is functional and has been extensively tested, however  and updating the code so please consider the benchmark kit as under development. \r\n\r\nPost your questions on BigBench Google group.\r\n\r\nhttps://groups.google.com/forum/#!forum/big-bench \r\n\r\nUNDER DEVELOPMENT -- Post your questions on Google Groups https://groups.google.com/forum/#!forum/big-bench\r\n for help any help in running the workload.\r\n \r\n To collect performance metrics from  Hadoop nodes and analyze the resource utilization draw automated charts using MS-Excel, PAT is available for download.\r\n \r\n https://github.com/intel-hadoop/PAT \r\n\r\n======\r\n\r\nThis document is a development version and describes the BigBench installation and execution on our AWS machines.\r\n\r\n# Preparation\r\n\r\n## Cluster Environment\r\n\r\n**Java**\r\n\r\nJava 1.7 is required. 64 bit is recommended\r\n\r\n**Hadoop**\r\n\r\n* Hive 0.12 recommended\r\n* Mahout\r\n\r\n## Installation\r\n\r\nOn the AWS installation, clone the github repository into a folder stored in $INSTALL_DIR:\r\n\r\n```\r\nexport INSTALL_DIR=\"$HOME\" # adapt this to your location\r\ncd $INSTALL_DIR\r\ngit clone https://<username>@github.com/intel-hadoop/Big-Bench.git\r\n```\r\n\r\n## Configuration\r\n\r\nCheck if the hadoop related variables are correctly set in the environment file:\r\n\r\n`vi \"$INSTALL_DIR/Big-Bench/setEnvVars\"`\r\n\r\nMajor settings, Specify your cluster environment:\r\n\r\n```\r\nBIG_BENCH_HADOOP_LIBS_NATIVE  (optional but speeds up hdfs access)\r\nBIG_BENCH_HADOOP_CONF         most important: core-site.xml and hdfs-site.xml\r\n```\r\nMinor settings:\r\n```\r\nBIG_BENCH_USER\r\nBIG_BENCH_DATAGEN_DFS_REPLICATION  replication count used during generation of the big bench table\r\nBIG_BENCH_DATAGEN_JVM_ENV          -Xmx750m is sufficient for Nodes with 2 CPU cores, remove or increase if your Nodes have more cores\r\n```\r\n# Run the workload\r\n\r\nThere are two different methods for running the workload: use the driver to simply perform a complete benchmark run or use the bash scripts to do partial tests. As the driver calls the bash scripts internally, both methods yield the same results.\r\n\r\n## Common hints\r\n\r\nThe following paragraphs are important for both methods.\r\n\r\n### Accept license\r\n\r\nWhen running the data generator for the first time, the user must accept its license:\r\n\r\n```\r\nBy using this software you must first agree to our terms of use. Press [ENTER] to show them\r\n... # license is displayed\r\nIf you have read and agree to these terms of use, please type (uppercase!): YES and press [ENTER]\r\nYES\r\n```\r\n\r\n### PDGF\r\nThe data are being generated directly into HDFS (into the $BIG_BENCH_HDFS_RELATIVE_INIT_DATA_DIR directory, absolute HDFS path is $BIG_BENCH_HDFS_ABSOLUTE_INIT_DATA_DIR).\r\n\r\nDefault HDFS replication count is 1 (data is only stored on the generating node). You can change this in the $BIG_BENCH_HOME/setEnvVars file by changing the variable\r\n`BIG_BENCH_DATAGEN_DFS_REPLICATION=<Replication count>'.\r\n\r\n## Using the BigBench driver\r\n\r\nThe BigBench driver is started with a script. To show all available options, you can call the help first:\r\n```\r\n\"$INSTALL_DIR/scripts/bigBench runBenchmark -h\r\n```\r\n\r\n### Quick start\r\n\r\nIf a complete benchmark run should be performed and no data were generated previously, this is the command which should be executed:\r\n\r\n```\r\n\"$INSTALL_DIR/scripts/bigBench runBenchmark -m <number of map tasks for data generation> -f <scale factor of dataset> -s <number of parallel streams in the throughput test>\r\n```\r\n\r\nThis command will generate data, run the load-, power- and throughput-test and calculate the BigBench result.\r\n\r\nSo a complete benchmark run with all stages can be done by running (e.g., 4 map tasks, scale factor 100, 2 streams):\r\n```\r\n\"$INSTALL_DIR/scripts/bigBench runBenchmark -m 4 -f 100 -s 2\r\n```\r\n\r\nAfter the benchmark finished, two log files are written: BigBenchResult.txt (which contains the driver's sysout messages) as well as BigBenchTimes.csv (which contains all measured timestamps/durations). The log directory can be specified with the -l option, it defaults to the BigBench's log dir ($BIG_BENCH_LOGS_DIR).\r\n\r\n### More detailed explanation\r\n\r\nThere are four phases the driver traverses (only three are benchmarked though): data generation, load test, power test and throughput test. The driver has a clean option (-c) which does not run the benchmark but rather cleans the environment from previous runs (if for some reason all generated data should be cleaned).\r\n\r\n#### Data generation\r\n\r\nThe data generation phase is not benchmarked by BigBench. The driver can skip this phase by setting \"-sd\". Skipping this phase is a good idea if data were already generated previously and the complete benchmark should be repeated with the same dataset size. In that case, generting data is not necessary as PDGF would generate the exact same data as in the previous run. If data generation is not skipped, two other options must be provided to the driver: \"-m\" sets the number of map tasks for PDGF's data generation, \"-f\" sets the scale factor determining the dataset size (1 scale factor equals 1 GiB). If \"-c\" is set and \"-sd\" is not set, the dataset directory in HDFS will be deleted.\r\n\r\n#### Load test\r\n\r\nPopulation of the hive metastore is the first phase that is benchmarked by BigBench. This phase can be skipped by providing \"-sl\" as an option. Re-populating the metastore is technically only necessary if the dataset has changed. Nevertheless, metastore population is part of the benchmark, so if this phase is skipped then no BigBench result can be computed. If \"-c\" is set and \"-sl\" is not set, the tables of the dataset in the metastore will be dropped (however it does not influence the query results from later tests).\r\n\r\n\r\n#### Power test\r\n\r\nThis is the second phase that is benchmarked by BigBench. All queries run sequentially in one stream. The phase can be skipped with the option \"-sp\". Setting \"-c\" (and not \"-sp\") cleans previous power-test's results in the hive metastore tables and HDFS directories.\r\n\r\n#### Throughput test\r\n\r\nThe throughput test is the last benchmark phase. All queries run in parallel streams in different order. The phase can be skipped with \"-st\". If this phase is not skipped, \"-s\" is a required option because that sets the number of parallel streams used in this phase. As in the other phases, setting \"-c\" (and not \"-st\") cleans the thoughput-test's results in the hive metastore and the HDFS directories.\r\n\r\n## Using the bigBench bash script\r\n\r\nThe driver internally calls the $BIG_BENCH_BASH_SCRIPT_DIR/bigBench bash script along with a module name. So every step the driver performs (apart from the more complicated \"query mixing\" and multi-stream execution logic) can be run manually by executing this script with the proper options.\r\n\r\n### Overview\r\n\r\nThe general syntax for the bigBench script is:\r\n```\r\n\"$INSTALL_DIR/scripts/bigBench [global options] moduleName [module options]\r\n```\r\n\r\nAt the moment there is only one module which processes module options itself, namely runBenchmark. All other modules currently do NO option processing. They rely on bigBench for option processing. Therefore when not running the runBenchmark module, global options must be specified.\r\n\r\nAll available options as well as all found modules can be listed by calling the script help:\r\n```\r\n\"$INSTALL_DIR/scripts/bigBench -h\r\n```\r\n\r\n### Available options\r\n* -b: This option chooses which binary will be used for the benchmark. WARNING: support for choosing other binaries than \"hive\" is implemented, but hive is the only tested binary. In fact, no other binary works so far. DO NOT USE THAT OPTION\r\n* -d: Some more complex queries are split into multiple internal parts. This option chooses which internal query part will be executed. This is a developer only option. ONLY USE IF YOU KNOW WHAT YOU ARE DOING\r\n* -f: The scale factor for PDGF. It is used by the clusterDataGen and hadoopDataGen modules\r\n* -h: Show help\r\n* -m: The map tasks used for data generation. It is used by the clusterDataGen and hadoopDataGen modules\r\n* -p: The benchmark phase to use. It is necessary if subsequent query runs should not overwrite results of previous queries. The driver internally uses POWER_TEST_IN_PROGRESS, THROUGHPUT_TEST_FIRST_QUERY_RUN_IN_PROGRESS and THROUGHPUT_TEST_SECOND_QUERY_RUN_IN_PROGRESS. The default value when not providing this option is RUN_QUERY\r\n* -q: Defines the query number to be executed\r\n* -s: This option defines the number of parallel streams to use. It is only of any use with the runQueryInParallel module\r\n* -t: Sets the stream number of the current query. This option is important so that one query can run multiple times in parallel without interfering with other instances\r\n* -v: Use the provided file as initial metastore population script\r\n* -w: Use the provided file as metastore refresh script\r\n* -y: Use the provided file for custom query parameters\r\n* -z: Use the provided file for custom hive settings\r\n\r\n### Modules usage examples\r\n\r\n* cleanData: cleans the dataset directory in HDFS. This module is automatically run by the data generator module to remove the dataset from the HDFS.\r\n```\r\n\"$INSTALL_DIR/scripts/bigBench cleanData\r\n```\r\n\r\n* cleanMetastore: cleans the metastore dataset tables.\r\n```\r\n\"$INSTALL_DIR/scripts/bigBench [-z <hive settings>] cleanMetastore\r\n```\r\n\r\n* cleanQueries: cleans all metastore tables and result directories in HDFS for all 30 queries. This module works as a wrapper for cleanQuery and does not work if \"-q\" is set as option.\r\n```\r\n\"$INSTALL_DIR/scripts/bigBench [-p <benchmark phase>] [-t <stream number] [-z <hive settings>] cleanQueries\r\n```\r\n\r\n* cleanQuery: cleans metastore tables and result directories in HDFS for one query. Needs the query number to be set.\r\n```\r\n\"$INSTALL_DIR/scripts/bigBench [-p <benchmark phase>] -q <query number> [-t <stream number] [-z <hive settings>] cleanQuery\r\n```\r\n\r\n* clusterDataGen: generates data using ssh on all defined nodes. This module is deprecated. Do not use it.\r\n\r\n* hadoopDataGen: generates data using a hadoop job. Needs the map tasks and scale factor options.\r\n```\r\n\"$INSTALL_DIR/scripts/bigBench -m <map tasks> -f <scale factor> hadoopDataGen\r\n```\r\n\r\n* populateMetastore: populates the metastore with the dataset tables.\r\n```\r\n\"$INSTALL_DIR/scripts/bigBench [-v <population script>] [-z <hive settings>] populateMetastore\r\n```\r\n\r\n* refreshMetastore: refreshes the metastore with the refresh dataset.\r\n```\r\n\"$INSTALL_DIR/scripts/bigBench [-w <refresh script>] [-z <hive settings>] refreshMetastore\r\n```\r\n\r\n* runBenchmark: runs the driver. This module parses its options itself. For details look at the driver usage section above.\r\n```\r\n\"$INSTALL_DIR/scripts/bigBench runBenchmark [driver options]\r\n```\r\n\r\n* runQueries: runs all 30 queries sequentially. This module works as a wrapper for runQuery and does not work if \"-q\" is set as option.\r\n```\r\n\"$INSTALL_DIR/scripts/bigBench [-p <benchmark phase>] [-t <stream number] [-z <hive settings>] runQueries\r\n```\r\n\r\n* runQuery: runs one query. Needs the query number to be set.\r\n```\r\n\"$INSTALL_DIR/scripts/bigBench [-p <benchmark phase>] -q <query number> [-t <stream number] [-z <hive settings>] runQuery\r\n```\r\n\r\n* runQueryInParallel: runs one query on multiple parallel streams. This module is a wrapper for runQuery. Needs the query number (\"-q\") and total number of streams (\"-s\") to be set and the stream number (\"-t\") to be unset.\r\n```\r\n\"$INSTALL_DIR/scripts/bigBench [-p <benchmark phase>] -q <query number> -s <number of parallel streams> [-z <hive settings>] runQueryInParallel\r\n```\r\n\r\n* showErrors: parses query errors in the log files after query runs.\r\n```\r\n\"$INSTALL_DIR/scripts/bigBench showErrors\r\n```\r\n\r\n* showTimes: parses execution times in the log files after query runs.\r\n```\r\n\"$INSTALL_DIR/scripts/bigBench showTimes\r\n```\r\n\r\n* zipQueryLogs: generates a zip file of all logs in the logs directory. It is run by the driver after each complete benchmark run. Subsequent runs override the old log files. A zip archive is created to save them before being overwritten.\r\n```\r\n\"$INSTALL_DIR/scripts/bigBench zipQueryLogs\r\n```\r\n\r\n# FAQ \r\nThis Benchmark does not favour any platform and we ran this benchmark on many different distributions. But you gotta start somewhere.\r\nThis Benchmark is also not HIVE specify, but hive happens to be the first module to be implemented.\r\n\r\nThis FAQ is mostly based on our experiments with Hive on Yarn with CDH 5.x)\r\n\r\n\r\n## Where do i put my cluster specific settings?\r\n=============================================\r\nHere: Big-Bench/setEnvVars\r\n\r\nWhere is my core-site.xml/hdfs-site.xml  for BIG_BENCH_HADOOP_CONF (usually the one in /etc/hadoop/...):\r\n\r\n`find / -name \"hdfs-site.xml\" 2> /dev/null`\r\n\r\nWhere is my hdfs native libs folder for BIG_BENCH_HADOOP_LIBS_NATIVE?\r\n\r\n`find / -name \"libhadoop.so\" 2> /dev/null`\r\n\r\n\r\nWhat is my name node address for BIG_BENCH_HDFS_NAMENODE? \r\n\r\nLook inside your hdfs-site.xml and locate this property value:\r\n``` \r\n<property>\r\n    <name>dfs.namenode.servicerpc-address</name>\r\n    <value>host.domain:8022</value>\r\n</property>\r\n```\r\n\r\n## Where do i put benchmark specific hive options?\r\nBig-Bench/hive/hiveSettings.sql\r\n\r\nThere are already a number of documented settings in there.\r\n\r\n\r\n## Where do i put query specific hive options?\r\nYou can place an optional file \"hiveLocalSettings.sql\" into a queries folder e.g.:\r\n\r\nBig-Bench/queries/q??/hiveLocalSettings.sql\r\n\r\nYou can put your query specific settings into this file, and the benchmark will automatically load the file. Its made so, that the hiveLocalSettings.sql file gets loaded last, which allows you to override any previously made settings made in e.g.: Big-Bench/hive/hiveSettings.sql\r\nThis way your settings are independent from github updates and there wont be any conflicts when updating the query files. \r\n\r\n\r\n## Underutilized cluster \r\n\r\n### cluster setup\r\nBefore \"tuning\" or asking in the google group, please ensure that your cluster is well configured and able to utilize all resources (cpu/mem/storage/netIO).\r\n\r\n\r\nThere are a lot of things you have to configure, depending on your hadoop distribution and your hardware.\r\nSome important variables regarding MapReduce task performance:\r\n```\r\nmapreduce.reduce.memory.mb\r\nmapreduce.map.memory.mb\r\nmapreduce.map.memory.mb\r\nmapreduce.map.memory.mb\r\nmapreduce.reduce.java.opts;\r\nmapreduce.map.java.opts\r\nmapreduce.map.java.opts\r\nmapreduce.task.io.sort.mb\r\nmapreduce.task.io.sort.mb\r\n...\r\n```\r\n\r\nBasically, you may want to have at least as much (yarn) \"containers\" (container may hold a map or a reduce task) on your cluster as you have CPU cores or hardware threads.\r\nDespite that, you configure your container count based on available memory in your cluster. 1-2GB of memory per container may be a good starting point.\r\n\r\nIn CDH you can do this with: (just example values! follow a more sophisticated tutorial on how to set up your cluster!):\r\n\r\n**Gateway**\r\nGateway BaseGroup --expand--> Resource management\r\n```\r\nContainer_Size (e.g.:  1,5Gb can be sufficient but you may require more if you run into \"OutOfMemory\" or \"GC overhead exceeded\" errors while executing this benchmark) \r\nmapreduce.map.memory.mb=Container_Size\r\nmapreduce.reduce.memory.mb=Container_Size\r\nmapreduce.map.java.opts.max.heap =0.75*Container_Size\r\nmapreduce.reduce.java.opts.max.heap =0.75*Container_Size\r\nClient Java Heap Size in Bytes =0.75*Container_Size\r\n```\r\n\r\n**Nodemanager**\r\nNodemanager BaseGroup --expand--> Resource management\r\n```\r\n -container memory\r\n how many memory ,all containers together, can allocate (physical \"free\" resources on nodes)\r\n - yarn.nodemanager.resource.cpu-vcores (same rules as container memory)\r\n```\r\n\r\n**ResourceManager**\r\nResourceManager BaseGroup --expand--> Resource management\r\n```\r\n -yarn.scheduler.minimum-allocation-mb  (set to 512mb or 1GB)\r\n -yarn.scheduler.maximum-allocation-mb  (hint: container memory/container max mem == minimum amount of containers per node)\r\n -yarn.scheduler.increment-allocation-mb  set to 512MB\r\n -yarn.scheduler.maximum-allocation-vcores  set to min amount of containers\r\n```\r\n\r\n**Dynamic resource pools**\r\n(cluster -> dynamic resource pools -> configuration)\r\n```\r\nIf everything runs fine, do not set anything here (no additional restrictions). \r\nIf you experience yarn deadlocks (yarn trying to allocate resources, but fails leading to MR-jobs waiting indefinitely for containers)  you may want set a limit.\r\n```\r\n\r\n\r\n### datagen stage: Tuning the DataGeneration tool\r\n\r\n**right settings for number of map tasks (bigBench -m option)**\r\n\r\nShort answer:\r\nOn map task per virtual CPU/hardware thread is best to utilize all CPU resources.\r\n\r\nBut settings in your cluster may not allow you executing this number of map tasks in parallel. Basically you can not run more parallel map tasks then available (yarn-) containers.\r\nAnother thing to consider when testing on big noisy clusters, is the non homogeneous runtime of nodes or node failures. Most mappers may finish long before certain others. To address for this skew in mapper runtime we suggest to set the number of mappers to a multiple (2-3 times) of available containers/threads in your cluster, reducing the runtime of a single mapper and making it cheaper to restart a task.\r\nBut be aware that a to short runtime per map tasks also hurts performance, because launching a task is associated with a considerable amount of overhead. In addition to that, more map tasks produce more intermediate files and thus causing more load for the HDFS namenode.\r\nTry targeting run times per mapper not shorter than 3 minutes.\r\n\r\nFor a \"small cluster\" (4nodes á 40 hardware threads) (4Nodes * 40Threads) * 2 = 320 MapTasks may be a good value.\r\n\r\n\r\n**advanced settings**\r\n\r\nIf your cluster has more available threads then concurrently runnable containers, your cluster may be CPU underutilized.\r\nIn this case you can increase the number of threads available to the data generation tool. The data generation tool will then allocate the specified number of threads per map task.\r\n\r\nPlease open you Big-Bench/setEnvVars configuration file and see lines:\r\n  export BIG_BENCH_DATAGEN_JVM_ENV=\" -Xmx300m \"        \r\nand:   \r\n  export BIGBENCH_DATAGEN_HADOOP_OPTIONS=\" -workers 1 -ap 3000 \"   \r\n\r\nYou could set\r\n  export BIGBENCH_DATAGEN_HADOOP_OPTIONS=\" -workers 4 -ap 3000 \"\r\ntelling the data generation tool to use 4 threads per map task.\r\nNote: increasing the number of threads  requires lager internal buffers so please add 100Mb of memory to  BIG_BENCH_DATAGEN_JVM_ENV per additional thread.\r\n\r\nYour final settings for 4 threads per map task should look like this:\r\n  export BIG_BENCH_DATAGEN_JVM_ENV=\" -Xmx600m \"\r\n  export BIGBENCH_DATAGEN_HADOOP_OPTIONS=\" -workers 4 -ap 3000 \" \r\n  \r\n\r\nOne map task per virtual CPU/hardware thread is best to utilize all CPU resources.\r\n\r\nBut settings in your cluster may not allow you executing this number of map tasks in parallel. Basically you can not run more parallel map tasks then available (yarn-) containers.\r\nAnother thing to consider when testing on big noisy cluster, is the non homogeneous runtime of nodes or node failures. Most mappers may finish long before certain others. To address for this skew in mapper runtime i would suggest to set the number of mappers to a multiple (2-3 times) of available containers/threads in your cluster, reducing the runtime of a single mapper and making it cheaper to restart a task.\r\nBut be aware that a to short runtime per map tasks also hurts performance. I would suggest targeting run times per mapper not shorter > 5min.\r\n\r\n\r\n\r\n\r\n### Hive \"loading\"-stage is slow,\r\nThe hive loading stage is not only \"moving\" file in hdfs from the data/ dir into the hive/warehouse.\r\n\r\nBig-Bench hive does not work on the plain CSV files, but instead transforms the files into the ORC file format, more efficient and native to hive.\r\nBig-Bench models a set of long running analytic queries. Thus it is more realistic not to store the tables in plain text format but in a optimized fashion.\r\n\r\nTransforming data into ORC is a very expensive tasks (index creation, compression, splitting and distributing/replication across the cluster) and loading the tables into hive is done by a single hive job. Since there are 23 distinct tables, hive will always create at least 23 hadoop jobs to do the CSV->ORC processing.\r\n\r\nYou could test if activating the following options on your cluster work for you:\r\nhive.exec.parallel=true\r\nhive.exec.parallel.thread.number=8\r\n\r\nThey allow hive to run multiple uncorrelated jobs in parallel (like creating tables). But be warned, this feature is still considered unstable (Hive 0.12).\r\nIf you cant modify your hive-site.xml cluster globally, you can uncomment/add these options in:\r\n  BigBench/hive/hiveCreateLoadORC.sql  \r\nto active them only for the loading stage or in:\r\n  Big-Bench/hive/hiveSettings.sql\r\nto enable them for the whole benchmark, including the queries.\r\n\r\n### Hive Query's are running slow\r\nUnfortunately there is no generic answer to this. \r\nFirst: this is a long running benchmark with hundreds of distinct mr-Jobs. Each mr-Job has a significant amount of \"scheduling\" overhead of around ~1minute. So even if you are only processing no data at all, you still have to pay the price of scheduling everything (which is arround 1,5 hours! for a single wave of all 30 queries).\r\nThere are several projects trying to reduces this problem like TEZ from the stinger imitative or Hive on Spark or SparkSQL. But with Hive on yarn, there is nothing you can really do about this.\r\n\r\n**Enough data (/bigBench -f <SF> option) ?**\r\n\r\nMake sure you run your benchmark with a big enough dataset to reduce the ratio of fixed overhead time vs. total runtime. Besides from initial testing your cluster setup, never run with a scaling factor of smaller than 100 ( -f 100 ==100GB)\r\n\r\n\r\n**Enough map/reduce tasks per query stage ?**\r\n\r\n\r\nLook in your logs and search for lines like this:\r\n```\r\nHadoop job information for Stage-2: number of mappers: 1; number of reducers: 1\r\n```\r\n\r\nIf the number of mappers/reducers is < than your available (yarn) slots or \"tasks\" you cluster is able to run (Rough estimate:  slots == number of CPU# in your cluster or: TotalClusterRAM/slotMaxMem), the query is not using all your clusters resources.\r\nBut don't generalize this. Some stages simply don't have enough data to justify more than 1 map job (e.g. the final stage of a \"... limit 100 SORT BY X;\" query only has to sort 100 lines).\r\nOr the processed table is just to small (like the time or date table).\r\nRemember that more map/reduce tasks also implies more overhead. So don't overdo it as to much map tasks can hurt performance just like to few tasks.\r\n\r\nYou can tune some parameters in the hive/hiveSettings.sql file. \r\nHive determines the number of map/reduce tasks based on the tables size. If you have a table of 670MB and set the max.split.size to 67000000 bytes, hive will start 10 map tasks to process this table (or maybe less if hive is able to reduce the dataset by using partitioning/bucketing)\r\n\r\n```\r\nset mapred.max.split.size=67108864;\r\nset mapred.min.split.size=1;\r\nset hive.exec.reducers.max=99999;\r\n```\r\n\r\n\r\n## More detailed log files\r\n\r\nThe aggregated yarn application log file created for a yarn job contains much more information than the default printout you see on your screen.\r\nThis log file is especially helpful to debug child-processes started by hadoop MR-jobs. e.g. java/pyhton scripts in certain streaming api using queries), or  the \"hadoopDataGeneration\" task which executes the data generator program.\r\n\r\nTo retrieve this log please follow these steps:\r\n\r\nIn your Big-Bench/logs/ folder files or on screen you will find a line similar to this:\r\n\r\n14/06/17 19:40:12 INFO impl.YarnClientImpl: Submitted application application_1403017220075_0022\r\n\r\nTo extract this line from the log file(s) execute:\r\n```\r\ngrep \"Submitted application\" ${BIG_BENCH_LOGS_DIR}/<log file of interest>.log\r\n```\r\n\r\nThe important part is the application ID (e.g. application_1403017220075_0022) itself.\r\nTake this ID and request the associate yarn log file using the following command line:\r\n```\r\nyarn logs -applicationId <applicationID>  > yarnApplicationLog.log\r\n```\r\n\r\n\r\n## Exceptions/Errors you may encounter\r\n\r\n\r\n### Execution of a MR Stage progresses quickly but then seems to \"hang\" at ~99%.\r\n\r\nThis indicates a skew in the data. This means: most reducers handle only very little data, and some (1-2) have to handle most of the data.  This happens if some keys are very frequent in comparison to others. \r\ne.g.: this is the case for user_sk in web_clickstreams. 50% of all clicks have user_sk == NULL (indicating that the click-stream did not result in a purchase).\r\nWhen a query uses the \"distribute by \" keyword, hive distributes the workload by this key. This implies that every reducer handles a specific set of keys. The single reducer responsible for the \"null\" key then effectively has to process >50% of the total workload (as 50% of all keys are null).\r\n\r\nWe did our best to filter out null keys within the querys, if the null values are irrelevant for the query result.  This does not imply that all hive querys are \"skew-free\". Hive offers some settings to tune this:\r\n```\r\nset hive.optimize.skewjoin=true;\r\nset hive.optimize.skewjoin.compiletime=true;\r\nset hive.groupby.skewindata=true;\r\nset hive.skewjoin.key=100000;\r\n-- read: https://issues.apache.org/jira/browse/HIVE-5888\r\n```\r\nBut be aware that turning on these options will produce worse! running times for data/queries that are not heavily skewed, which is the reason they are disabled by default. \r\n\r\n\r\n### Execution failed with exit status: 3\r\n```\r\nExecution failed with exit status: 3\r\nFAILED: Execution Error, return code 3 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask\r\n```\r\n\r\nHive converted a join into a locally running and faster 'mapjoin', but ran out of memory while doing so.\r\nThere are two bugs responsible for this.\r\n\r\n\r\n**Bug 1)**\r\n\r\nhives metric for converting joins miscalculated the required amount of memory. This is especially true for compressed files and ORC files, as hive uses the filesize as metric, but compressed tables require more memory in their uncompressed 'in memory representation'.\r\n\r\nYou could simply decrease 'hive.smalltable.filesize' to tune the metric, or increase 'hive.mapred.local.mem' to allow the allocation of more memory for map tasks.\r\n\r\nThe later option may lead to bug number two if you happen to have a affected hadoop version.\r\n\r\n**Bug 2)**\r\n\r\nHive/Hadoop ignores  'hive.mapred.local.mem' !\r\n(more exactly: bug in Hadoop 2.2 where hadoop-env.cmd sets the -xmx parameter multiple times, effectively overriding the user set hive.mapred.locla.mem setting. \r\nsee: https://issues.apache.org/jira/browse/HADOOP-10245\r\n\r\n**There are 3 workarounds for this bug:**\r\n\r\n* 1) assign more memory to the local! Hadoop JVM client (this is not! mapred.map.memory) because map-join child jvm will inherit the parents jvm settings\r\n * In cloudera manager home, click on \"hive\" service,\r\n * then on the hive service page click on \"configuration\"\r\n * Gateway base group --(expand)--> Resource Management -> Client Java Heap Size in Bytes -> 1GB \r\n* 2) reduce \"hive.smalltable.filesize\" to ~1MB or below (depends on your cluster settings for the local JVM)\r\n* 3) turn off \"hive.auto.convert.join\" to prevent hive from converting the joins to a mapjoin.\r\n\r\n2) & 3) can be set in Big-Bench/hive/hiveSettings.sql\r\n\r\n\r\n### Cannot allocate memory\r\n\r\n```\r\nCannot allocate memory\r\nThere is insufficient memory for the Java Runtime Environment to continue.\r\n```\r\n\r\nNative memory allocation (malloc) failed to allocate x bytes for committing reserved memory. \r\n\r\nBasically your kernel handed out more memory than actually available, in expectants that most programs actually never use (allocate) every last bit of memory they request. Now a program (in this case java) tries to allocate something in its virtual reserved memory area, but the kernel was wrong with his estimation of application memory consumption and there is no physical memory left available to fulfil the applications malloc request.\r\nhttp://www.oracle.com/technetwork/articles/servers-storage-dev/oom-killer-1911807.html\r\n\r\n**WARNING:**\r\nSome \"fixes\" suggest disabling  \"vm.overcommit_memory\" in the kernel.\r\nIf you are already in an \"overcommitted\" state DO NOT SET sysctl vm.overcommit_memory=2 on the running machine to \"cure\" it! If you do, you will no longer be able to execute ANY program or shell command, as this would require a memory allocation of which nothing is left. This essentially will deadlock you machine, requiring you to forcefully physically reboot the system.\r\n\r\n\r\n### java.io.IOException: Exceeded MAX_FAILED_UNIQUE_FETCHES;\r\n```\r\njava.io.IOException: Exceeded MAX_FAILED_UNIQUE_FETCHES;\r\nbailing-out.\r\n```\r\n\r\nThis cryptic exception basically translates to:\r\nCould not communicate with  node(s). Tried to copy results between nodes but\r\nwe failed after to many retries.\r\n\r\nCauses:\r\n* some nodes cannot communicate between each other\r\n* disturbed network\r\n* some node terminated\r\n\r\n\r\n###  Caused by: java.lang.InstantiationException: org.apache.hadoop.hive.ql.parse.ASTNodeOrigin ### \r\n* https://issues.apache.org/jira/browse/HIVE-6765\r\n* https://issues.apache.org/jira/browse/HIVE-5068\r\n\r\n\r\n### Error: GC overhead limit exceeded\r\n```\r\nDiagnostic Messages for this Task:\r\nError: GC overhead limit exceeded\r\n\r\nFAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask\r\n```\r\nNot enough (remote) mapper/reducer memory to complete the job.\r\nYou have to increase your mapper/reducer job memory limits (and/or yarn container limits).\r\n\r\nPlease read the chapter **cluster setup** from this FAQ section.\r\n\r\nNote that this error is different from:\r\n```\r\nExecution failed with exit status: 3\r\nFAILED: Execution Error, return code 3 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask\r\n```\r\nas \"Exit status: 3\" indicates a memory overflow in the \"LOCAL\" jvm (the jvm that started your hive task) where as \"Error, return code 2\" indicates a \"REMOTE\" problem. (A jvm started by e.g. YARN on a Node to process your job)\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}