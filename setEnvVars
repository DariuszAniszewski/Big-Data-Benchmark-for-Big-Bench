## determine base directory
cd "$(dirname ${BASH_SOURCE[0]})"
export BIG_BENCH_HOME="$PWD"
cd "$OLDPWD"

## ================================================================
##  ADAPT BELOW VARIABLES TO YOUR CLUSTER
## ================================================================


## ==========================
## HADOOP environment
## ==========================
                  
##folder containing the cluster setup *-site.xml files like core-site.xml 
export BIG_BENCH_HADOOP_CONF="/etc/hadoop/conf.cloudera.hdfs"      

export BIG_BENCH_HADOOP_LIBS_NATIVE="/usr/lib/hadoop/lib/native"

## memory used by sub-processes spawned by Hive queries (like streaming M/R jobs etc.)  
## Suggestion for value: (YarnConatiner_MB - hive_MB)*0.7 e.g. (2000Mb-500Mb)*0.7=1050
export BIG_BENCH_java_child_process_xmx=" -Xmx1024m "				

##NOT USED BY BENCHMARK! options only used by: scripts/mountHdfs.sh
export BIG_BENCH_HDFS_MOUNT_POINT="/mnt/hdfs"
export BIG_BENCH_HDFS_NAMENODE="bb-aws1.us-west-2.compute.internal:8022"

## ==========================
## HDFS config and paths
## ==========================
export BIG_BENCH_USER="$USER"
export BIG_BENCH_HDFS_ABSOLUTE_PATH="/user/$BIG_BENCH_USER"		 ##working dir of benchmark. 
export BIG_BENCH_HDFS_REAL_BASE_DIR="$BIG_BENCH_HDFS_MOUNT_POINT$BIG_BENCH_HDFS_ABSOLUTE_PATH"
export BIG_BENCH_HDFS_RELATIVE_INIT_DATA_DIR="benchmarks/bigbench/data"
export BIG_BENCH_HDFS_RELATIVE_REFRESH_DATA_DIR="benchmarks/bigbench/data_refresh"
export BIG_BENCH_HDFS_RELATIVE_QUERY_RESULT_DIR="benchmarks/bigbench/queryResults"
export BIG_BENCH_HDFS_RELATIVE_TEMP_DIR="benchmarks/bigbench/temp"
export BIG_BENCH_HDFS_ABSOLUTE_INIT_DATA_DIR="$BIG_BENCH_HDFS_ABSOLUTE_PATH/$BIG_BENCH_HDFS_RELATIVE_INIT_DATA_DIR"
export BIG_BENCH_HDFS_ABSOLUTE_REFRESH_DATA_DIR="$BIG_BENCH_HDFS_ABSOLUTE_PATH/$BIG_BENCH_HDFS_RELATIVE_REFRESH_DATA_DIR"
export BIG_BENCH_HDFS_ABSOLUTE_QUERY_RESULT_DIR="$BIG_BENCH_HDFS_ABSOLUTE_PATH/$BIG_BENCH_HDFS_RELATIVE_QUERY_RESULT_DIR"
export BIG_BENCH_HDFS_ABSOLUTE_TEMP_DIR="$BIG_BENCH_HDFS_ABSOLUTE_PATH/$BIG_BENCH_HDFS_RELATIVE_TEMP_DIR"

## ==========================
## HIVE (recommended minimum version v0.12)
## ==========================
export BIG_BENCH_HIVE_DATABASE="bigbenchORC"
export BIG_BENCH_hive_default_fileformat_source_table="ORC"
export BIG_BENCH_hive_default_fileformat_result_table="TEXTFILE"

export HIVE_BINARY="/usr/bin/hive"
export BIG_BENCH_SHARK_HOME="/var/lib/spark/shark-0.9.1"
export SHARK_BINARY="$BIG_BENCH_SHARK_HOME/bin/shark-withinfo"
export DEFAULT_BINARY="$HIVE_BINARY"
#export DEFAULT_BINARY="$SHARK_BINARY"

## ================================================================
##  /END/  ADAPT BELOW VARIABLES TO YOUR CLUSTER
## ================================================================


## ===================================================
## important subdirectories
## ===================================================
## if you are using "bigBenchClusterDataGen.sh", the following locations must be reachable from all Nodes. 
##option a) place the folder in you home and replicate it on every node 
##option b) place folder into a shared location like a network file system 
##option c) simply mount the hdfs as fuse-file-system (on every node) and place your BIG_BENCH_HOME folder it there

export BIG_BENCH_BASH_SCRIPT_DIR="$BIG_BENCH_HOME/scripts"
export BIG_BENCH_DATA_GENERATOR_DIR="$BIG_BENCH_HOME/data-generator"
export BIG_BENCH_HIVE_SCRIPT_DIR="$BIG_BENCH_HOME/hive"
export BIG_BENCH_QUERIES_DIR="$BIG_BENCH_HOME/queries"
export BIG_BENCH_TOOLS_DIR="$BIG_BENCH_HOME/tools"
export BIG_BENCH_LOGS_DIR="$BIG_BENCH_HOME/logs"
export BIG_BENCH_LOADING_STAGE_LOG="$BIG_BENCH_LOGS_DIR/hiveLoading.log"
export BIG_BENCH_DATAGEN_STAGE_LOG="$BIG_BENCH_LOGS_DIR/dataGeneration.log"

## ===================================================
## Data generation options
## ===================================================

#(recommended:1) replication count for files written by the datagenerator to HDFS into dir: BIG_BENCH_HDFS_ABSOLUTE_INIT_DATA_DIR and BIG_BENCH_HDFS_ABSOLUTE_REFRESH_DATA_DIR
export BIG_BENCH_DATAGEN_DFS_REPLICATION="1" 			

# if empty, generate all tables. 
# Else: specify tables to generate e.g.: BIG_BENCH_DATAGEN_TABLES="item customer store"
# Tables to choose from: customer customer_address customer_demographics date_dim household_demographics income_band inventory item item_marketprices product_reviews promotion reason ship_mode store store_returns store_sales time_dim warehouse web_clickstreams web_page  web_returns web_sales web_site
export BIG_BENCH_DATAGEN_TABLES=""										


#do not touch lines below, unless you are sure you need to tamper with that
export BIG_BENCH_DATAGEN_CORE_SITE="$BIG_BENCH_HADOOP_CONF/core-site.xml"
export BIG_BENCH_DATAGEN_HDFS_SITE="$BIG_BENCH_HADOOP_CONF/hdfs-site.xml"


# --------------------------------------------
# Hadoop data gen options
# --------------------------------------------
# specify JVM arguments like: -Xmx2000m; 
# default of: 300m is sufficient if the datagen only uses 1 worker thread per map task 
# Add +100MB per addition worker if you modified: BIGBENCH_DATAGEN_HADOOP_OPTIONS
export BIG_BENCH_HADOOP_DATAGEN_JVM_ENV="java -DDEBUG_PRINT_PERIODIC_THREAD_DUMPS=5000 -Xmx300m " 		


# if you increase -workers, you must also increase the -Xmx setting in BIG_BENCH_DATAGEN_JVM_ENV; 		
#-ap:=automatic progress ,3000ms intervall; prevents hadoop from killing long running jobs
#-workers:=limit hadoop based data generator to use 1 CPU core per map task. 
export BIGBENCH_DATAGEN_HADOOP_OPTIONS=" -workers 1 -ap 3000 "	


# --------------------------------------------
# scripts/bigBenchClusterDataGen.sh options
# --------------------------------------------
# BIG_BENCH_NODES can be a list of hostnames as well as a path to a file containing hostnames
export BIG_BENCH_NODES="localhost"		
	

## SSH options
BIG_BENCH_SSH_OPTIONS=""					   ## you want to make sure that you can reach all nodes in BIG_BENCH_NODES Via ssh. use this variable to specify the key file to ssh (e.g.: SSH_OPTIONS="-i <keyfile>")

# specify JVM arguments like: -Xmx2000m
# 300m is sufficient if the datagen only uses 1 worker thread (default: unrestricted)
# Calculation: 200Mb +(100MB*threads per node)
export BIG_BENCH_DATAGEN_JVM_ENV="java " 	  



## ===================================================
## END OF SETTINGS
## ===================================================


## check BIG_BENCH_NODES variable for file path
# Do not change the following code
if [ -r "$BIG_BENCH_NODES" ]
then
	while read host
	do
		HOSTLIST="$HOSTLIST $host"
	done < "$BIG_BENCH_NODES"
	export BIG_BENCH_NODES="$HOSTLIST"
fi


## ===================================================
## Set permissions
## ===================================================
find $BIG_BENCH_HOME -name '*.sh' -exec chmod 755 {} +
find $BIG_BENCH_HOME -name '*.jar' -exec chmod 755 {} +

##===================================================
## add bash script dir to path (if not already present)
##===================================================

if ! echo $PATH | grep "$BIG_BENCH_BASH_SCRIPT_DIR" > /dev/null 2>&1
then
  export PATH="$BIG_BENCH_BASH_SCRIPT_DIR:$PATH"
fi

logEnvInformation ()
{
	local ENV_INFO_FILE="$BIG_BENCH_LOGS_DIR/envInfo.log"
	[ ! -d "$BIG_BENCH_LOGS_DIR" ] && mkdir -p "$BIG_BENCH_LOGS_DIR"
	[ ! -f "$ENV_INFO_FILE" ] && ( hadoop version; hadoop classpath; java -version; ) > "$ENV_INFO_FILE" 2>&1
}
